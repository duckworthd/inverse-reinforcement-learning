'''
Created on Apr 7, 2011

@author: duckworthd
'''
from util.classes import NumMap
from mdp.agent import Agent, MapAgent, QValueAgent
from numpy import dot, outer, zeros, eye
import numpy.random
from numpy.linalg import pinv
import random
from mdp import simulation

class ExactSolver(object):
    def solve(self, mdp):
        '''Returns an Agent directed by a policy determined by this solver'''
        raise NotImplementedError()
    
class ApproximateSolver(object):
    def solve(self, mdp):
        '''Approximately solve an MDP via samples generated by simulation.
        a linear reward function is assume, and reward_f is thus assumed to
        be of class LinearReward.  Only its feature function will be used.'''
        raise NotImplementedError()    
    
class ValueIterator(ExactSolver):
    def __init__(self, max_iter):
        self._max_iter = max_iter
        
    def solve(self, mdp):
        V = NumMap()
        for i in range(self._max_iter):
            V = self.iter(mdp, V, 'max')
        return MapAgent(self.iter(mdp, V, 'argmax'))
        
    @classmethod
    def iter(cls, mdp, V, max_or_argmax='max'):
        ''' 1 step lookahead via the Bellman Update.  final argument should
        be either 'max' or 'argmax', determining whether a state-value function
        or a policy is returned'''
        if max_or_argmax == 'max':
            V_next = NumMap()
        else:
            V_next = {}
        for s in mdp.S():
            if mdp.is_terminal(s):
                V[s] = 0.0
                continue
            q = NumMap()    # Q-states for state s
            for a in mdp.A(s):
                r = mdp.R(s,a)
                T = mdp.T(s,a)
                expected_rewards = [T[s_prime]*V[s_prime] for s_prime in T]
                q[a] = r + mdp.gamma*sum(expected_rewards)
            if max_or_argmax == 'max':
                V_next[s] = q.max()
            else:
                V_next[s] = q.argmax()
        return V_next

class LinearQValueAgent(Agent):
    '''Implicitly encodes a policy using an MDP's feature function
    and available actions for a given state along with its own weights'''

    def __init__(self,weights, mdp):
        self._w = weights
        self._mdp = mdp
    
    def actions(self,state):
        actions = NumMap()
        for a in self._mdp.A(state):
            phi = self._mdp.reward_function.features(state, a)
            actions[a] = dot( phi, self._w )
        result = NumMap()
        result[actions.argmax()] = 1.0
        return result
        
class LSPI(ApproximateSolver):
    '''Least Squares Policy Iteration (Lagoudakis, Parr 2001)'''
    
    def __init__(self, n_iter, n_samples):
        self._n_iter = n_iter
        self._n_samples = n_samples
        
    def solve(self, mdp, initial=None):
        # initialize policy randomly
        k = mdp.reward_function.dim
        w = numpy.random.rand( k )
        agent = LinearQValueAgent(w, mdp)

        # initial state distribution
        if initial == None:
            s = random.choice(mdp.S())
            initial = NumMap( {s:1.0} )
        
        for i in range(self._n_iter):
            # Generate samples
            samples = simulation.simulate(mdp, agent, initial, self._n_samples)
            
            # evaluate policy approximately
            w = self.value_weights(samples, mdp.reward_function, mdp.gamma, agent)
            
            ###
#            for s in mdp.S():
#                for a in mdp.A(s):
#                    print 'Q(%s,%s) = %f' % (s,a,dot( mdp.reward_function.features(s,a), w ))
            ###
            
            # Define an agent to use argmax over Q(s,a) to choose actions
            agent = LinearQValueAgent(w,mdp)
             
        return LinearQValueAgent(w,mdp)
    
    def value_weights(self, samples, reward_f, gamma, agent):
        '''find weights to approximate value function of a given policy
        Q(s,a) ~~ dot(w,phi(s,a))'''
#        k = reward_f.dim
#        B = (1/0.001)*eye(k)
#        b = zeros(k)
#        for i in range(len(samples)-1):
#            (s,a,r) = samples[i]
#            s_prime = samples[i+1][0]
#            phi = reward_f.features(s,a)
#            phi_prime = reward_f.features(s_prime, agent.sample(s_prime))
#            B = B - dot( dot(B,outer(phi,phi-gamma*phi_prime)), B )/(1+dot(phi-gamma*phi_prime, dot(B, phi)))
#            b = b + phi*r
#        w = dot(B,b)
#        return w
        k = reward_f.dim
        A = zeros( [k,k] )
        b = zeros( k )
        for i in range(len(samples)-1):
            (s,a,r) = samples[i]
            s_prime = samples[i+1][0]
            phi = reward_f.features(s,a)
            phi_prime = reward_f.features(s_prime, agent.sample(s_prime))
            A = A + outer(phi,phi - gamma*phi_prime)
            b = b + phi*r
        w = dot( pinv(A), b)
        return w